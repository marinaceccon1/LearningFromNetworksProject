{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marinaceccon1/LearningFromNetworksProject/blob/main/Degree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computation with the second approach and the degree centrality"
      ],
      "metadata": {
        "id": "LfxIFrQZc4ty"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6jTjSDHScNpR"
      },
      "outputs": [],
      "source": [
        "#import the libraries needed\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import sys\n",
        "import collections\n",
        "import random\n",
        "from builtins import sum as siumm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clone the github repository containing the dataset\n",
        "!git clone https://github.com/marinaceccon1/LearningFromNetworksProject.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ITqFjyVchRu",
        "outputId": "8d06f8f6-2c0a-49d3-f14a-dfcf788cc722"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'LearningFromNetworksProject' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data and create the graph representing the network\n",
        "matrix = np.loadtxt('/content/LearningFromNetworksProject/out.txt', usecols=range(2))\n",
        "G = nx.from_edgelist(matrix)"
      ],
      "metadata": {
        "id": "VWhW_RRccjQo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computes the local clustering coefficient and order the dictionaries w.r.t. the keys\n",
        "lcc = nx.clustering(G)\n",
        "ordered_lcc = collections.OrderedDict(sorted(lcc.items()))"
      ],
      "metadata": {
        "id": "l2t6EcoqcpMe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computes the degree centrality and order the dictionaries w.r.t. the keys\n",
        "deg_centralities = nx.degree_centrality(G)\n",
        "ordered_deg_centralities = collections.OrderedDict(sorted(deg_centralities.items()))"
      ],
      "metadata": {
        "id": "gAClXeXZcxYs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sort the dictionary in incresing order w.r.t. values(reverse = False)\n",
        "sorted_lcc = sorted(ordered_lcc.items(), key=lambda x:x[1], reverse = False) #sorted_lcc is a list of pairs (key, value)"
      ],
      "metadata": {
        "id": "HHS9eH7ccxgk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save in len_sorted_lcc the number of values below the threshold\n",
        "for i in range(len(sorted_lcc)):\n",
        "  if sorted_lcc[i][1] > 0.25:\n",
        "    len_sorted_lcc = i\n",
        "    break"
      ],
      "metadata": {
        "id": "edgiGH8GdR0V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sort the dictionary in decreasing order w.r.t. the values(reverse = True)\n",
        "sorted_deg_centr = sorted(ordered_deg_centralities.items(),  key=lambda x:x[1], reverse = True)"
      ],
      "metadata": {
        "id": "MQRvVL10dEBW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save in len_sorted_deg_centr the number of values above the threshold\n",
        "len_sorted_deg_centr = 0\n",
        "for i in range(len(sorted_deg_centr)):\n",
        "  if sorted_deg_centr[i][1] < 0.00005:\n",
        "    len_sorted_deg_centr = i\n",
        "    break"
      ],
      "metadata": {
        "id": "r4lwf0ItdEKp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creates a list with the first len_sorted_lcc keys in the sorted_lcc list\n",
        "new_sorted_lcc = []\n",
        "for i in range(len_sorted_lcc):\n",
        "  new_sorted_lcc.append(sorted_lcc[i][0])\n",
        "s1 = set(new_sorted_lcc) #converts the list into a set, s1 eventually contains the indices of the nodes below the threshold"
      ],
      "metadata": {
        "id": "U8ROWuLxdEW_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates a list wit the first len_sorted_deg_centr keys in sorted_deg_centr\n",
        "new_sorted_bet_centr = []\n",
        "for i in range(len_sorted_deg_centr):\n",
        "  new_sorted_bet_centr.append(sorted_deg_centr[i][0])\n",
        "s2 = set(new_sorted_bet_centr) #converts the list into a set, s2 eventually contains the indices of the nodes above the threshold"
      ],
      "metadata": {
        "id": "9DdHsorOjg_J"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computes the intersection\n",
        "s = s1 & s2"
      ],
      "metadata": {
        "id": "I8WVydHmjhI9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_list = list(s)"
      ],
      "metadata": {
        "id": "IUDAbyemjhTr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cardinalities = {} #new dictionary, the keys will be pairs of nodes that are not adjacent, the value is the cardinality \n",
        "#of the intersection of their neighbourhoods\n",
        "\n",
        "for i in range(len(node_list)): #we consider all the nodes in this list, i.e. all the nodes with associated metrics within the thresholds\n",
        "  for j in range(i+1,len(node_list)): #in order not to contemplate the same pair twice, first as (i,j) and after as (j,i), we set j>i\n",
        "    \n",
        "    breakout_flag = False #needed to compute a double break\n",
        "    for key,value in (G.adj[node_list[i]]).items(): #check the neighbours of node_list[j]     \n",
        "      if key == node_list[j]: #if inverse_sorted_ratio[j][0] is a neighbour of node_list[i]\n",
        "        breakout_flag = True\n",
        "        break #exit the for key,value loop\n",
        "    if breakout_flag:\n",
        "      break #exit the for j loop\n",
        "\n",
        "    s1 = set(G.adj[node_list[i]]) #convert the dictionary in a set containing all the keys in the dictionary\n",
        "    s2 = set(G.adj[node_list[j]])\n",
        "    res = s1 & s2 #keys that belong to both sets\n",
        "\n",
        "    cardinalities[(node_list[i], node_list[j])] = len(res) #the value associated to that pair \n",
        "    #of nodes is the cardinality of the intersection computed above\n",
        "\n",
        "inverse_sorted_card = sorted(cardinalities.items(), key=lambda x:x[1], reverse = True) #order the dictionary by value in decreasing order"
      ],
      "metadata": {
        "id": "fD9x8ajGl3mg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 500"
      ],
      "metadata": {
        "id": "vhhlhM60l3zV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add the first N edges in inverse_sorted_card to a copy of the graph\n",
        "G_1 = G.copy()\n",
        "for i in range(N):\n",
        "  u = inverse_sorted_card[i][0][0]\n",
        "  v = inverse_sorted_card[i][0][1]\n",
        "  G_1.add_edge(u,v)\n",
        "\n",
        "glo_clu_coeff = nx.transitivity(G_1) #compute the global clustering coefficient of the new graph\n",
        "triangles = siumm(nx.triangles(G_1).values())/3 #compute the number of triangles of the new graph\n",
        "print(glo_clu_coeff) \n",
        "print(triangles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQyeKS3_l358",
        "outputId": "9e7745c8-c1f3-43c0-f68b-ab17d93e47e7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14806849469334796\n",
            "3518478.0\n"
          ]
        }
      ]
    }
  ]
}